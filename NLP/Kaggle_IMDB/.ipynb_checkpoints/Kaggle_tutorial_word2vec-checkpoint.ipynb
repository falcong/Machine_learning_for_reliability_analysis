{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read 25000 labeled train reviews, 25000 labeled test reviews,  and 50000 unlabeled reviews\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import Preprocessing_nlp as pre\n",
    "#from NLP.Kaggle_IMDB.Preprocessing_nlp import review_to_words\n",
    "\n",
    "###  1. import the data###\n",
    "\n",
    "data_path = '/home/peng/Documents/NLP/Kaggle_datasets/'\n",
    "\n",
    "train = pd.read_csv(data_path + 'labeledTrainData.tsv', header = 0, \\\n",
    "                    delimiter = '\\t', quoting = 3)\n",
    "test = pd.read_csv(data_path + 'testData.tsv', header = 0, \\\n",
    "                    delimiter = '\\t', quoting = 3)\n",
    "unlabeled_train = pd.read_csv(data_path + 'unlabeledTrainData.tsv', header = 0, \\\n",
    "                    delimiter = '\\t', quoting = 3)\n",
    "\n",
    "#verify the shape\n",
    "print 'Read %d labeled train reviews, %d labeled test reviews, ' \\\n",
    "' and %d unlabeled reviews\\n' %(train['review'].size, test['review'].size, unlabeled_train['review'].size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing sentences from training set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/peng/anaconda2/lib/python2.7/site-packages/bs4/__init__.py:198: UserWarning: \".\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
      "  '\"%s\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.' % markup)\n",
      "/home/peng/anaconda2/lib/python2.7/site-packages/bs4/__init__.py:207: UserWarning: \"http://www.happierabroad.com\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  '\"%s\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client to get the document behind the URL, and feed that document to Beautiful Soup.' % markup)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing sentences from unlabeled set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/peng/anaconda2/lib/python2.7/site-packages/bs4/__init__.py:207: UserWarning: \"http://www.archive.org/details/LovefromaStranger\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  '\"%s\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client to get the document behind the URL, and feed that document to Beautiful Soup.' % markup)\n",
      "/home/peng/anaconda2/lib/python2.7/site-packages/bs4/__init__.py:207: UserWarning: \"http://www.loosechangeguide.com/LooseChangeGuide.html\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  '\"%s\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client to get the document behind the URL, and feed that document to Beautiful Soup.' % markup)\n",
      "/home/peng/anaconda2/lib/python2.7/site-packages/bs4/__init__.py:207: UserWarning: \"http://www.msnbc.msn.com/id/4972055/site/newsweek/\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  '\"%s\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client to get the document behind the URL, and feed that document to Beautiful Soup.' % markup)\n",
      "/home/peng/anaconda2/lib/python2.7/site-packages/bs4/__init__.py:198: UserWarning: \"..\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
      "  '\"%s\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.' % markup)\n",
      "/home/peng/anaconda2/lib/python2.7/site-packages/bs4/__init__.py:207: UserWarning: \"http://www.youtube.com/watch?v=a0KSqelmgN8\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  '\"%s\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client to get the document behind the URL, and feed that document to Beautiful Soup.' % markup)\n",
      "/home/peng/anaconda2/lib/python2.7/site-packages/bs4/__init__.py:207: UserWarning: \"http://jake-weird.blogspot.com/2007/08/beneath.html\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  '\"%s\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client to get the document behind the URL, and feed that document to Beautiful Soup.' % markup)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "795538\n",
      "[u'with', u'all', u'this', u'stuff', u'going', u'down', u'at', u'the', u'moment', u'with', u'mj', u'i', u've', u'started', u'listening', u'to', u'his', u'music', u'watching', u'the', u'odd', u'documentary', u'here', u'and', u'there', u'watched', u'the', u'wiz', u'and', u'watched', u'moonwalker', u'again']\n"
     ]
    }
   ],
   "source": [
    "## 2. preprocess the data\n",
    "import Preprocessing_nlp as pre\n",
    "'''\n",
    ":word2vec does not need remove stop words,  because the algorithm relies on the broader \n",
    "context of the sentence in order to produce high-quality word vectors.\n",
    "\n",
    ":word2vec expects single sentences, each one as a list of words a list of lists\n",
    "'''\n",
    "## 2.1  use NLTK's punkt tokenizer for sentence splitting\n",
    "import nltk.data\n",
    "\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "\n",
    "sentences = []\n",
    "\n",
    "print 'Parsing sentences from training set'\n",
    "for review in train['review']:\n",
    "    sentences += pre.review_to_sentences(review.decode(\"utf8\"), tokenizer)\n",
    "\n",
    "print \"Parsing sentences from unlabeled set\"\n",
    "for review in unlabeled_train[\"review\"]:\n",
    "    sentences += pre.review_to_sentences(review.decode(\"utf8\"), tokenizer)\n",
    "    \n",
    "    \n",
    "print len(sentences)\n",
    "print sentences[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using gpu device 0: GeForce GTX 980 Ti (CNMeM is enabled with initial size: 35.0% of memory, cuDNN 5005)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model...\n"
     ]
    }
   ],
   "source": [
    "#3. Convert clean data to vectors  Word2vec !!!!!\n",
    "\n",
    "# 3.1 Import the built-in logging module and configure it so that Word2Vec \n",
    "# creates nice output messages\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s',\\\n",
    "    level=logging.INFO)\n",
    "\n",
    "# Set values for various parameters\n",
    "num_features = 300    # Word vector dimensionality                      \n",
    "min_word_count = 40   # Minimum word count                        \n",
    "num_workers = 4       # Number of threads to run in parallel\n",
    "context = 10          # Context window size                                                                                    \n",
    "downsampling = 1e-3   # Downsample setting for frequent words\n",
    "\n",
    "# Initialize and train the model (this will take some time)\n",
    "from gensim.models import word2vec\n",
    "print \"Training model...\"\n",
    "model = word2vec.Word2Vec(sentences, workers=num_workers, \\\n",
    "            size=num_features, min_count = min_word_count, \\\n",
    "            window = context, sample = downsampling)\n",
    "\n",
    "# If you don't plan to train the model any further, calling \n",
    "# init_sims will make the model much more memory-efficient.\n",
    "model.init_sims(replace=True)\n",
    "\n",
    "# It can be helpful to create a meaningful model name and \n",
    "# save the model for later use. You can load it later using Word2Vec.load()\n",
    "model_name = \"300features_40minwords_10context\"\n",
    "model.save(data_path + model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pillow\n",
      "[(u'jews', 0.7370978593826294), (u'muslims', 0.7369353175163269), (u'germans', 0.721481442451477), (u'indians', 0.7140231132507324), (u'europeans', 0.7108433246612549), (u'blacks', 0.7043591737747192), (u'christians', 0.6873964667320251), (u'immigrants', 0.6870779991149902), (u'whites', 0.6853542327880859), (u'africans', 0.6624138951301575)]\n"
     ]
    }
   ],
   "source": [
    "#4. Exploring the model results\n",
    "\n",
    "'''\n",
    "The \"doesnt_match\" function will try to deduce which word in a set is most dissimilar from the others:\n",
    "'''\n",
    "\n",
    "print model.doesnt_match('pillow bed desk kitchen'.split())\n",
    "\n",
    "print model.most_similar('americans')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<type 'numpy.ndarray'>\n",
      "(16490, 300)\n",
      "(300,)\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Part 3 More fun with word vectors\n",
    "'''\n",
    "\n",
    "# 1. Numeric representation of words\n",
    "from gensim.models import Word2Vec\n",
    "model = Word2Vec.load(data_path+model_name)\n",
    "\n",
    "print type(model.syn0)\n",
    "print model.syn0.shape\n",
    "\n",
    "'''\n",
    "The number of rows in syn0 is the number of words in the model's vocabulary, and \n",
    "the number of columns corresponds to the size of the feature vector, \n",
    "'''\n",
    "print model['flower'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'module' object has no attribute 'getAvgFeatureVecs'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-27-7a57da7a5695>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[0mclean_train_reviews\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m \u001b[0mpre\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreview_to_wordlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreview\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mremove_stopwords\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m \u001b[0mtrainDataVecs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpre\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetAvgFeatureVecs\u001b[0m\u001b[1;33m(\u001b[0m \u001b[0mclean_train_reviews\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_features\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m \u001b[1;32mprint\u001b[0m \u001b[1;34m'finished'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'module' object has no attribute 'getAvgFeatureVecs'"
     ]
    }
   ],
   "source": [
    "#2. Average each reviews to the same shape of feature sets\n",
    "\n",
    "'''\n",
    "Since each word is a vector in 300-dimensional space, we can use vector operations to combine the words in\n",
    "each review. One method we tried was to simply average the word vectors in a given review (for this purpose,\n",
    "we removed stop words, which would just add noise).\n",
    "'''\n",
    "import Preprocessing_nlp as pre\n",
    "\n",
    "clean_train_reviews = []\n",
    "for review in train['review']:\n",
    "    clean_train_reviews.append( pre.review_to_wordlist(review, remove_stopwords = True))\n",
    "    \n",
    "trainDataVecs = pre.getAvgFeatureVecs( clean_train_reviews, model, num_features)    \n",
    "print 'finished'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
