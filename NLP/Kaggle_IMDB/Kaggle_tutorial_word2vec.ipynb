{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read 25000 labeled train reviews, 25000 labeled test reviews,  and 50000 unlabeled reviews\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('/home/peng/git/Machine_learning_for_reliability_analysis/Preprocess')\n",
    "sys.path.append('/home/peng/git/Machine_learning_for_reliability_analysis/NLP')\n",
    "import pandas as pd\n",
    "import Preprocessing_nlp as pre\n",
    "#from NLP.Kaggle_IMDB.Preprocessing_nlp import review_to_words\n",
    "\n",
    "###  1. import the data###\n",
    "\n",
    "data_path = '/home/peng/Documents/NLP/Kaggle_datasets/'\n",
    "\n",
    "train = pd.read_csv(data_path + 'labeledTrainData.tsv', header = 0, \\\n",
    "                    delimiter = '\\t', quoting = 3)\n",
    "test = pd.read_csv(data_path + 'testData.tsv', header = 0, \\\n",
    "                    delimiter = '\\t', quoting = 3)\n",
    "unlabeled_train = pd.read_csv(data_path + 'unlabeledTrainData.tsv', header = 0, \\\n",
    "                    delimiter = '\\t', quoting = 3)\n",
    "\n",
    "#verify the shape\n",
    "print 'Read %d labeled train reviews, %d labeled test reviews, ' \\\n",
    "' and %d unlabeled reviews\\n' %(train['review'].size, test['review'].size, unlabeled_train['review'].size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"With all this stuff going down at the moment with MJ i've started listening to his music, watching the odd documentary here and there, watched The Wiz and watched Moonwalker again. Maybe i just want to get a certain insight into this guy who i thought was really cool in the eighties just to maybe make up my mind whether he is guilty or innocent. Moonwalker is part biography, part feature film which i remember going to see at the cinema when it was originally released. Some of it has subtle messages about MJ's feeling towards the press and also the obvious message of drugs are bad m'kay.<br /><br />Visually impressive but of course this is all about Michael Jackson so unless you remotely like MJ in anyway then you are going to hate this and find it boring. Some may call MJ an egotist for consenting to the making of this movie BUT MJ and most of his fans would say that he made it for the fans which if true is really nice of him.<br /><br />The actual feature film bit when it finally starts is only on for 20 minutes or so excluding the Smooth Criminal sequence and Joe Pesci is convincing as a psychopathic all powerful drug lord. Why he wants MJ dead so bad is beyond me. Because MJ overheard his plans? Nah, Joe Pesci's character ranted that he wanted people to know it is he who is supplying drugs etc so i dunno, maybe he just hates MJ's music.<br /><br />Lots of cool things in this like MJ turning into a car and a robot and the whole Speed Demon sequence. Also, the director must have had the patience of a saint when it came to filming the kiddy Bad sequence as usually directors hate working with one kid let alone a whole bunch of them performing a complex dance scene.<br /><br />Bottom line, this movie is for people who like MJ on one level or another (which i think is most people). If not, then stay away. It does try and give off a wholesome message and ironically MJ's bestest buddy in this movie is a girl! Michael Jackson is truly one of the most talented people ever to grace this planet but is he guilty? Well, with all the attention i've gave this subject....hmmm well i don't know because people can be different behind closed doors, i know this for a fact. He is either an extremely nice but stupid guy or one of the most sickest liars. I hope he is not the latter.\"\n"
     ]
    }
   ],
   "source": [
    "print train['review'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing sentences from training set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/peng/anaconda2/lib/python2.7/site-packages/bs4/__init__.py:166: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "To get rid of this warning, change this:\n",
      "\n",
      " BeautifulSoup([your markup])\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup([your markup], \"lxml\")\n",
      "\n",
      "  markup_type=markup_type))\n",
      "/home/peng/anaconda2/lib/python2.7/site-packages/bs4/__init__.py:198: UserWarning: \".\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
      "  '\"%s\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.' % markup)\n",
      "/home/peng/anaconda2/lib/python2.7/site-packages/bs4/__init__.py:207: UserWarning: \"http://www.happierabroad.com\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  '\"%s\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client to get the document behind the URL, and feed that document to Beautiful Soup.' % markup)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing sentences from unlabeled set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/peng/anaconda2/lib/python2.7/site-packages/bs4/__init__.py:207: UserWarning: \"http://www.archive.org/details/LovefromaStranger\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  '\"%s\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client to get the document behind the URL, and feed that document to Beautiful Soup.' % markup)\n",
      "/home/peng/anaconda2/lib/python2.7/site-packages/bs4/__init__.py:207: UserWarning: \"http://www.loosechangeguide.com/LooseChangeGuide.html\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  '\"%s\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client to get the document behind the URL, and feed that document to Beautiful Soup.' % markup)\n",
      "/home/peng/anaconda2/lib/python2.7/site-packages/bs4/__init__.py:207: UserWarning: \"http://www.msnbc.msn.com/id/4972055/site/newsweek/\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  '\"%s\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client to get the document behind the URL, and feed that document to Beautiful Soup.' % markup)\n",
      "/home/peng/anaconda2/lib/python2.7/site-packages/bs4/__init__.py:198: UserWarning: \"..\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
      "  '\"%s\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.' % markup)\n",
      "/home/peng/anaconda2/lib/python2.7/site-packages/bs4/__init__.py:207: UserWarning: \"http://www.youtube.com/watch?v=a0KSqelmgN8\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  '\"%s\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client to get the document behind the URL, and feed that document to Beautiful Soup.' % markup)\n",
      "/home/peng/anaconda2/lib/python2.7/site-packages/bs4/__init__.py:207: UserWarning: \"http://jake-weird.blogspot.com/2007/08/beneath.html\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  '\"%s\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client to get the document behind the URL, and feed that document to Beautiful Soup.' % markup)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "795538\n",
      "[u'with', u'all', u'this', u'stuff', u'going', u'down', u'at', u'the', u'moment', u'with', u'mj', u'i', u've', u'started', u'listening', u'to', u'his', u'music', u'watching', u'the', u'odd', u'documentary', u'here', u'and', u'there', u'watched', u'the', u'wiz', u'and', u'watched', u'moonwalker', u'again']\n"
     ]
    }
   ],
   "source": [
    "## 2. preprocess the data\n",
    "import Preprocessing_nlp as pre\n",
    "\n",
    "num_reviews = train['review'].size\n",
    "clean_train_reviews = []\n",
    "\n",
    "for i in xrange(0, num_reviews):\n",
    "    \n",
    "    if ((i+1)%5000 == 0):\n",
    "        print \"review %d of %d\\n\" % (i+1, num_reviews)\n",
    "    \n",
    "    clean_train_reviews.append(pre.review_to_list(train['review'][i]))\n",
    "\n",
    "\n",
    "'''\n",
    ":word2vec does not need remove stop words,  because the algorithm relies on the broader \n",
    "context of the sentence in order to produce high-quality word vectors.\n",
    "\n",
    ":word2vec expects single sentences, each one as a list of words a list of lists\n",
    "'''\n",
    "## 2.1  use NLTK's punkt tokenizer for sentence splitting\n",
    "import nltk.data\n",
    "\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "\n",
    "sentences = []\n",
    "\n",
    "print 'Parsing sentences from training set'\n",
    "for review in train['review']:\n",
    "    sentences += pre.review_to_sentences(review.decode(\"utf8\"), tokenizer)\n",
    "\n",
    "print \"Parsing sentences from unlabeled set\"\n",
    "for review in unlabeled_train[\"review\"]:\n",
    "    sentences += pre.review_to_sentences(review.decode(\"utf8\"), tokenizer)\n",
    "    \n",
    "    \n",
    "print len(sentences)\n",
    "print sentences[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using gpu device 0: GeForce GTX 980 Ti (CNMeM is enabled with initial size: 35.0% of memory, cuDNN 5005)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model...\n"
     ]
    }
   ],
   "source": [
    "#3. Convert clean data to vectors  Word2vec !!!!!\n",
    "\n",
    "# 3.1 Import the built-in logging module and configure it so that Word2Vec \n",
    "# creates nice output messages\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s',\\\n",
    "    level=logging.INFO)\n",
    "\n",
    "# Set values for various parameters\n",
    "num_features = 300    # Word vector dimensionality                      \n",
    "min_word_count = 40   # Minimum word count                        \n",
    "num_workers = 4       # Number of threads to run in parallel\n",
    "context = 10          # Context window size                                                                                    \n",
    "downsampling = 1e-3   # Downsample setting for frequent words\n",
    "\n",
    "# Initialize and train the model (this will take some time)\n",
    "from gensim.models import word2vec\n",
    "print \"Training model...\"\n",
    "model = word2vec.Word2Vec(sentences, workers=num_workers, \\\n",
    "            size=num_features, min_count = min_word_count, \\\n",
    "            window = context, sample = downsampling)\n",
    "\n",
    "# If you don't plan to train the model any further, calling \n",
    "# init_sims will make the model much more memory-efficient.\n",
    "model.init_sims(replace=True)\n",
    "\n",
    "# It can be helpful to create a meaningful model name and \n",
    "# save the model for later use. You can load it later using Word2Vec.load()\n",
    "model_name = \"300features_40minwords_10context\"\n",
    "model.save(data_path + model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pillow\n",
      "[(u'jews', 0.7370978593826294), (u'muslims', 0.7369353175163269), (u'germans', 0.721481442451477), (u'indians', 0.7140231132507324), (u'europeans', 0.7108433246612549), (u'blacks', 0.7043591737747192), (u'christians', 0.6873964667320251), (u'immigrants', 0.6870779991149902), (u'whites', 0.6853542327880859), (u'africans', 0.6624138951301575)]\n"
     ]
    }
   ],
   "source": [
    "#4. Exploring the model results\n",
    "\n",
    "'''\n",
    "The \"doesnt_match\" function will try to deduce which word in a set is most dissimilar from the others:\n",
    "'''\n",
    "\n",
    "print model.doesnt_match('pillow bed desk kitchen'.split())\n",
    "\n",
    "print model.most_similar('americans')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using gpu device 0: GeForce GTX 980 Ti (CNMeM is enabled with initial size: 35.0% of memory, cuDNN 5005)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<type 'numpy.ndarray'>\n",
      "(16490, 300)\n",
      "(300,)\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Part 3 More fun with word vectors\n",
    "'''\n",
    "\n",
    "# 1. Numeric representation of words\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "model_name = \"300features_40minwords_10context\"\n",
    "num_features = 300   \n",
    "model = Word2Vec.load(data_path+model_name)\n",
    "\n",
    "print type(model.syn0)\n",
    "print model.syn0.shape\n",
    "\n",
    "'''\n",
    "The number of rows in syn0 is the number of words in the model's vocabulary, and \n",
    "the number of columns corresponds to the size of the feature vector, \n",
    "'''\n",
    "print model['flower'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/peng/anaconda2/lib/python2.7/site-packages/bs4/__init__.py:166: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "To get rid of this warning, change this:\n",
      "\n",
      " BeautifulSoup([your markup])\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup([your markup], \"lxml\")\n",
      "\n",
      "  markup_type=markup_type))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "review 0 of 25000\n",
      "review 1000 of 25000\n",
      "review 2000 of 25000\n",
      "review 3000 of 25000\n",
      "review 4000 of 25000\n",
      "review 5000 of 25000\n",
      "review 6000 of 25000\n",
      "review 7000 of 25000\n",
      "review 8000 of 25000\n",
      "review 9000 of 25000\n",
      "review 10000 of 25000\n",
      "review 11000 of 25000\n",
      "review 12000 of 25000\n",
      "review 13000 of 25000\n",
      "review 14000 of 25000\n",
      "review 15000 of 25000\n",
      "review 16000 of 25000\n",
      "review 17000 of 25000\n",
      "review 18000 of 25000\n",
      "review 19000 of 25000\n",
      "review 20000 of 25000\n",
      "review 21000 of 25000\n",
      "review 22000 of 25000\n",
      "review 23000 of 25000\n",
      "review 24000 of 25000\n",
      "finished\n"
     ]
    }
   ],
   "source": [
    "#2. Average each reviews to the same shape of feature sets\n",
    "\n",
    "'''\n",
    "Since each word is a vector in 300-dimensional space, we can use vector operations to combine the words in\n",
    "each review. One method we tried was to simply average the word vectors in a given review (for this purpose,\n",
    "we removed stop words, which would just add noise).\n",
    "'''\n",
    "import Preprocessing_nlp as pre\n",
    "\n",
    "clean_train_reviews = []\n",
    "for review in train['review']:\n",
    "    clean_train_reviews.append( pre.review_to_wordlist(review, remove_stopwords = True))\n",
    "    \n",
    "trainDataVecs = pre.getAvgFeatureVecs( clean_train_reviews, model, num_features)    \n",
    "print 'finished'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating average feature vecs for test reviews\n",
      "review 0 of 25000\n",
      "review 1000 of 25000\n",
      "review 2000 of 25000\n",
      "review 3000 of 25000\n",
      "review 4000 of 25000\n",
      "review 5000 of 25000\n",
      "review 6000 of 25000\n",
      "review 7000 of 25000\n",
      "review 8000 of 25000\n",
      "review 9000 of 25000\n",
      "review 10000 of 25000\n",
      "review 11000 of 25000\n",
      "review 12000 of 25000\n",
      "review 13000 of 25000\n",
      "review 14000 of 25000\n",
      "review 15000 of 25000\n",
      "review 16000 of 25000\n",
      "review 17000 of 25000\n",
      "review 18000 of 25000\n",
      "review 19000 of 25000\n",
      "review 20000 of 25000\n",
      "review 21000 of 25000\n",
      "review 22000 of 25000\n",
      "review 23000 of 25000\n",
      "review 24000 of 25000\n"
     ]
    }
   ],
   "source": [
    "print 'creating average feature vecs for test reviews'\n",
    "clean_test_reviews = []\n",
    "for review in test['review']:\n",
    "    clean_test_reviews.append( pre.review_to_wordlist( review, remove_stopwords = True))\n",
    "    \n",
    "testDataVecs = pre.getAvgFeatureVecs( clean_test_reviews, model, num_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished\n"
     ]
    }
   ],
   "source": [
    "#3. Fit a random forest to the training data, using 100 trees\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import pandas as pd\n",
    "forest = RandomForestClassifier(n_estimators = 100)\n",
    "\n",
    "fprest = forest.fit(trainDataVecs, train['sentiment'])\n",
    "\n",
    "result = forest.predict (testDataVecs)\n",
    "\n",
    "output = pd.DataFrame( data = {'id': test['id'], 'sentiment': result})\n",
    "output.to_csv ( data_path + 'Word2Vec_AverageVectors.csv', index = False, quoting = 3)\n",
    "print 'finished'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16490, 300)\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "The result is underperformed bag of words, since elementg-wise average of vectors didnot produce\n",
    "spectacular results.\n",
    "\n",
    "'''\n",
    "import numpy as np\n",
    "\n",
    "print np.shape(model.syn0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here\n"
     ]
    }
   ],
   "source": [
    "#Part 4 Clustering word2vectors\n",
    "\n",
    "'''\n",
    "Word2Vec creates clusters of semantically related words, so another possible approach is to exploit the similarity\n",
    "of words within a cluster. Grouping vectors in this way is known as vector quantization. Using kmeans to find it.\n",
    "\n",
    "Trial and error suggested that small clusters, with an average of only 5 words or so per cluster, gave \n",
    "better results\n",
    "'''\n",
    "from sklearn.cluster import KMeans\n",
    "import time\n",
    "\n",
    "start = time.time()\n",
    "#set 'k' to be 1/5th of the vocabulary size, or an average of 5 words per cluster\n",
    "word_vectors = model.syn0\n",
    "num_clusters = word_vectors.shape[0]/5\n",
    "\n",
    "#initialize a k-means\n",
    "kmeans_clustering = KMeans( n_clusters = num_clusters)\n",
    "idx = kmeans_clustering.fit_predict( word_vectors)\n",
    "print 'here'\n",
    "end = time.time()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time is  13.7213349144 min\n"
     ]
    }
   ],
   "source": [
    "elapsed = end - start\n",
    "print 'time is ', elapsed/60, 'min'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cluster 0\n",
      "[u'irritation', u'unworthy', u'annoyance']\n",
      "\n",
      "Cluster 1\n",
      "[u'matt', u'bauer', u'reggie', u'lois', u'clark', u'randall', u'jack', u'cain']\n",
      "\n",
      "Cluster 2\n",
      "[u'waterfront', u'beaches']\n",
      "\n",
      "Cluster 3\n",
      "[u'pidgeon', u'walter', u'brennan']\n",
      "\n",
      "Cluster 4\n",
      "[u'parenting', u'repression', u'boldly', u'bourgeoisie', u'globalization', u'dogma', u'rhetoric', u'decadence', u'backlash', u'excesses', u'metaphysical', u'crusades', u'feminism', u'decadent', u'highlighting', u'unity']\n",
      "\n",
      "Cluster 5\n",
      "[u'arabs', u'ban', u'u', u'turks', u'nazis', u'russians']\n",
      "\n",
      "Cluster 6\n",
      "[u'floor', u'door', u'ceiling', u'wall', u'table']\n",
      "\n",
      "Cluster 7\n",
      "[u'appropriate', u'suitable', u'fitting', u'acceptable', u'apt']\n",
      "\n",
      "Cluster 8\n",
      "[u'unlikeable', u'loathsome', u'unsympathetic', u'unattractive', u'singularly', u'furthermore', u'despicable', u'colorless', u'miscast', u'unappealing', u'unlikable']\n",
      "\n",
      "Cluster 9\n",
      "[u'brainwashed', u'welcomed', u'rescued', u'threatened', u'summoned', u'befriended', u'rejected', u'attacked', u'dumped', u'mistreated', u'trained', u'stalked', u'enslaved', u'kidnapped', u'pursued', u'recruited', u'defeated', u'harassed', u'chased', u'resurrected', u'abducted', u'hunted']\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "The cluster assignment for each word is now stored in idx, and the vocabulary from our original \n",
    "Word2Vec model is still stored in model.index2word. For convenience, we zip these into one dictionary as follows:\n",
    "'''\n",
    "# Create a Word / Index dictionary, mapping each vocabulary word to\n",
    "# a cluster number                                                                                            \n",
    "word_centroid_map = dict(zip( model.index2word, idx ))\n",
    "# For the first 10 clusters\n",
    "for cluster in xrange(0,10):\n",
    "    #\n",
    "    # Print the cluster number  \n",
    "    print \"\\nCluster %d\" % cluster\n",
    "    #\n",
    "    # Find all of the words for that cluster number, and print them out\n",
    "    words = []\n",
    "    for i in xrange(0,len(word_centroid_map.values())):\n",
    "        if( word_centroid_map.values()[i] == cluster ):\n",
    "            words.append(word_centroid_map.keys()[i])\n",
    "    print words\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16490,)\n"
     ]
    }
   ],
   "source": [
    "print np.shape(model.index2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'module' object has no attribute 'create_bag_of_centroids'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-30-d5ab13aae84f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[0mcounter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mreview\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mclean_train_reviews\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m     \u001b[0mtrain_centroids\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcounter\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpre\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreate_bag_of_centroids\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreview\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword_centroid_map\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m     \u001b[0mcounter\u001b[0m \u001b[1;33m+=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'module' object has no attribute 'create_bag_of_centroids'"
     ]
    }
   ],
   "source": [
    "'''\n",
    "At any rate, now we have a cluster (or \"centroid\") assignment for each word, and we can define a \n",
    "function to convert reviews into bags-of-centroids. This works just like Bag of Words but uses semantically \n",
    "related clusters instead of individual words:\n",
    "'''\n",
    "\n",
    "#pre-allocate an array for the training set bags of centroids (for spped)\n",
    "\n",
    "train_centroids = np.zeros((train['review'].size, num_clusters), dtype = 'float32')\n",
    "\n",
    "#transform the trainng set reviews into bags of centroids\n",
    "\n",
    "counter = 0\n",
    "for review in clean_train_reviews:\n",
    "    train_centroids[counter]=pre.create_bag_of_centroids(review, word_centroid_map)\n",
    "    counter +=1\n",
    "    \n",
    "\n",
    "# Repeat for test reviews \n",
    "test_centroids = np.zeros(( test[\"review\"].size, num_clusters), \\\n",
    "    dtype=\"float32\" )\n",
    "\n",
    "counter = 0\n",
    "for review in clean_test_reviews:\n",
    "    test_centroids[counter] = create_bag_of_centroids( review, \\\n",
    "        word_centroid_map )\n",
    "    counter += 1\n",
    "\n",
    "print 'finished'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
