{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named nn",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-97ca1c5f0497>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mpickle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0msys\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mscipy\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mscipy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mio\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msequence\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnp_utils\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSequential\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: No module named nn"
     ]
    }
   ],
   "source": [
    "import os,pickle,sys,numpy,nn,copy,scipy,scipy.io\n",
    "import numpy as np\n",
    "from keras.preprocessing import sequence\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.optimizers import sgd\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.layers import Convolution2D, MaxPooling2D\n",
    "from keras.layers import SimpleRNN\n",
    "from keras.initializations import normal, identity\n",
    "import json\n",
    "import timeit\n",
    "\n",
    "start = timeit.default_timer()\n",
    "# --------------------------------------------\n",
    "# Parameters\n",
    "# --------------------------------------------\n",
    "root_folder_path = '/home/peng/Desktop/ Pynotebooks/Link to Machine_learning_for_reliability_analysis/QSAR/model_weihts/'\n",
    "seed  = 3453\n",
    "#print sys.argv\n",
    "splits = np.arange(0,1)\n",
    "\n",
    "mb    = 25     # size of the minibatch\n",
    "hist  = 0.1    # fraction of the history to be remembered\n",
    "\n",
    "hidden_size = 300\n",
    "\n",
    "dataset = scipy.io.loadmat('/home/peng/Documents/Project_C/QSAR_nlp/qm7.mat')\n",
    "for split in splits:\n",
    "\tprint \"this is the %d split\"%split\n",
    "\n",
    "\t# --------------------------------------------\n",
    "\t# Extract training data\n",
    "\t# --------------------------------------------\n",
    "\tP = dataset['P'][range(0,split)+range(split+1,5)].flatten()\n",
    "\t\n",
    "\tx_train = dataset['X'][P]\n",
    "\t\t\n",
    "\tx_label = dataset['T'][0,P]\n",
    "\t\n",
    "\tprint np.shape(x_label)\n",
    "\n",
    "\t# --------------------------------------------\n",
    "\t### Convert it to 1D for irnn\n",
    "\tx_train = x_train.reshape(x_train.shape[0], -1)\n",
    "\t#x_train /= x_train.reshape(x_train.shape[0], -1, 1)\n",
    "\tx_train = x_train.astype('float32')\n",
    "\tx_label = x_label.astype('float32')\n",
    "\tprint np.shape(x_train)\n",
    "\t\n",
    "\t# --------------------------------------------\n",
    "\t# Design the DNN model\n",
    "\t# --------------------------------------------\n",
    "\n",
    "\n",
    "\tmodel = Sequential()\n",
    "\tmodel.add(Dense(output_dim=hidden_size,\n",
    "\t                    #init=lambda shape, name: normal(shape, scale=0.001, name=name),\n",
    "\t                    #inner_init=lambda shape, name: identity(shape, scale=1.0, name=name),\n",
    "\t                    activation='relu',\n",
    "\t                    input_shape=x_train.shape[1:]))\n",
    "\tmodel.add(Dense(hidden_size, activation='relu'))\n",
    "\tmodel.add(Dense(hidden_size, activation='relu'))\n",
    "\tmodel.add(Dense(1, activation='linear'))\n",
    "\tmodel.compile(loss=\"mean_squared_error\", optimizer=\"rmsprop\",verbose=False)\n",
    "\n",
    "\tmodel.fit(x_train, x_label, batch_size=mb, nb_epoch=15),time.sleep(0.2)\n",
    "\n",
    "   \n",
    "    \n",
    "    ###design DNN and fit\n",
    "\n",
    "\n",
    "\t###save the weights\n",
    "\n",
    "\tmodel.save_weights(root_folder_path + 'nn_keras_%d'%split + \".h5\", overwrite=True)\t\n",
    "\twith open(root_folder_path + 'nn_keras_%d'%split + \".json\", \"w\") as outfile:\t\t\n",
    "\t    json.dump(model.to_json(), outfile)\n",
    "\n",
    "stop = timeit.default_timer()\n",
    "\n",
    "print (\"The running takes %r min\" %((stop-start)/60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "test set:\n",
      "[[ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " ..., \n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]]\n",
      "finished\n"
     ]
    }
   ],
   "source": [
    "import os,pickle,sys,numpy,copy,scipy,scipy.io\n",
    "import numpy as np\n",
    "import json\n",
    "from keras.models import model_from_json\n",
    "from keras.preprocessing import sequence\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.optimizers import sgd\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.layers import Convolution2D, MaxPooling2D\n",
    "from keras.layers import SimpleRNN\n",
    "from keras.initializations import normal, identity\n",
    "import json\n",
    "\n",
    "\n",
    "root_folder_path = '/home/peng/Desktop/ Pynotebooks/Link to Machine_learning_for_reliability_analysis/QSAR/model_weihts/'\n",
    "# --------------------------------------------\n",
    "# Parameters\n",
    "# --------------------------------------------\n",
    "#split = int(sys.argv[1]) # test split for cross-validation (between 0 and 5)\n",
    "splits = np.arange(0,1)\n",
    "# --------------------------------------------\n",
    "# Load data and models\n",
    "# --------------------------------------------\n",
    "#if not os.path.exists('qm7.mat'): os.system('wget http://www.quantum-machine.org/data/qm7.mat')\n",
    "dataset = scipy.io.loadmat('/home/peng/Documents/Project_C/QSAR_nlp/qm7.mat')\n",
    "acc_mae = 0.0\n",
    "\n",
    "for split in splits:\n",
    "\twith open('/home/peng/Desktop/ Pynotebooks/Link to Machine_learning_for_reliability_analysis/QSAR/model_weihts/nn_keras%d'%split + \".json\", 'r') as jfile:\n",
    "\t\tmodel = model_from_json(json.load(jfile))\n",
    "\tmodel.load_weights('/home/peng/Desktop/ Pynotebooks/Link to Machine_learning_for_reliability_analysis/QSAR/model_weihts/nn_keras%d'%split + '.h5')\n",
    "\tmodel.compile(\"sgd\", \"mse\")\n",
    "\t\n",
    "#\tprint('results after %d iterations'%nn.nbiter)\n",
    "\t\n",
    "\tPtrain = dataset['P'][range(0,split)+range(split+1,5)].flatten()\n",
    "\t#Ptest  = dataset['P'][split]\n",
    "\tPtest  = dataset['P'][split-1]\n",
    "\tfor P,name in zip([Ptest],['test']):\n",
    "\t\t# --------------------------------------------\n",
    "\t\t# Extract test data\n",
    "\t\t# --------------------------------------------\n",
    "\t\tx_train = dataset['X'][P]\n",
    "\t\tx_train = x_train.reshape(x_train.shape[0], -1, 1)\n",
    "\t\tx_label = dataset['T'][0,P]\n",
    "\t\n",
    "\t\t# --------------------------------------------\n",
    "\t\t# Test the neural network\n",
    "\t\t# --------------------------------------------\n",
    "\t\tprint('\\n%s set:'%name)\n",
    "\t\tpredict_label = model.predict(x_train)\n",
    "        print predict_label\n",
    "\t\t\n",
    "\t\t\n",
    "\t\t\n",
    "\t\t#print('MAE:  %5.2f kcal/mol'%numpy.abs(np.float(predict_label)-np.float(x_label).mean(axis=0))\n",
    "\n",
    "print ('finished')              \n",
    "              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
