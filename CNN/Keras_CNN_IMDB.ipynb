{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "20000 train sequences\n",
      "5000 test sequences\n"
     ]
    }
   ],
   "source": [
    "'''This example demonstrates the use of Convolution1D for text classification.\n",
    "Gets to 0.89 test accuracy after 2 epochs.\n",
    "90s/epoch on Intel i5 2.4Ghz CPU.\n",
    "10s/epoch on Tesla K40 GPU.\n",
    "'''\n",
    "\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "np.random.seed(1337)  # for reproducibility\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation,Flatten\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Convolution1D, MaxPooling1D\n",
    "from keras.datasets import imdb\n",
    "from keras import backend as K\n",
    "\n",
    "\n",
    "# set parameters:\n",
    "max_features = 5000\n",
    "maxlen = 400\n",
    "batch_size = 32\n",
    "embedding_dims = 50\n",
    "nb_filter = 250\n",
    "filter_length = 3\n",
    "hidden_dims = 250\n",
    "nb_epoch = 2\n",
    "\n",
    "print('Loading data...')\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(nb_words=max_features)\n",
    "print(len(X_train), 'train sequences')\n",
    "print(len(X_test), 'test sequences')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pad sequences (samples x time)\n",
      "X_train shape: (20000, 400)\n",
      "X_test shape: (5000, 400)\n"
     ]
    }
   ],
   "source": [
    "print('Pad sequences (samples x time)')\n",
    "X_train_1 = sequence.pad_sequences(X_train, maxlen=maxlen)\n",
    "X_test_1 = sequence.pad_sequences(X_test, maxlen=maxlen)\n",
    "print('X_train shape:', X_train_1.shape)\n",
    "print('X_test shape:', X_test_1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20000,)\n",
      "(20000, 400)\n",
      "[1, 20, 28, 716, 48, 495, 79, 27, 493, 8, 2, 7, 50, 5, 4682, 2, 10, 5, 852, 157, 11, 5, 1716, 3351, 10, 5, 500, 2, 6, 33, 256, 41, 2, 7, 17, 23, 48, 1537, 3504, 26, 269, 929, 18, 2, 7, 2, 4284, 8, 105, 5, 2, 182, 314, 38, 98, 103, 7, 36, 2184, 246, 360, 7, 19, 396, 17, 26, 269, 929, 18, 1769, 493, 6, 116, 7, 105, 5, 575, 182, 27, 5, 1002, 1085, 130, 62, 17, 24, 89, 17, 13, 381, 1421, 8, 2, 7, 5, 2723, 38, 325, 7, 17, 23, 93, 9, 156, 252, 19, 235, 20, 28, 5, 104, 76, 7, 17, 169, 35, 2, 17, 23, 1460, 7, 36, 2184, 934, 56, 2134, 6, 17, 891, 214, 11, 5, 1552, 6, 92, 6, 33, 256, 82, 7]\n",
      "[   0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    1   20   28  716   48  495   79   27  493    8\n",
      "    2    7   50    5 4682    2   10    5  852  157   11    5 1716 3351   10\n",
      "    5  500    2    6   33  256   41    2    7   17   23   48 1537 3504   26\n",
      "  269  929   18    2    7    2 4284    8  105    5    2  182  314   38   98\n",
      "  103    7   36 2184  246  360    7   19  396   17   26  269  929   18 1769\n",
      "  493    6  116    7  105    5  575  182   27    5 1002 1085  130   62   17\n",
      "   24   89   17   13  381 1421    8    2    7    5 2723   38  325    7   17\n",
      "   23   93    9  156  252   19  235   20   28    5  104   76    7   17  169\n",
      "   35    2   17   23 1460    7   36 2184  934   56 2134    6   17  891  214\n",
      "   11    5 1552    6   92    6   33  256   82    7]\n"
     ]
    }
   ],
   "source": [
    "print (np.shape(X_train))\n",
    "print (np.shape(X_train_1))\n",
    "print (X_train[0])\n",
    "print (X_train_1[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 275, 1538, 18, 2, 72, 242, 139, 6, 18, 5, 191, 525, 21, 2, 2228, 2056, 75, 18, 6, 19, 158, 566, 545, 10, 55, 50, 72, 824, 6, 2, 8, 234, 1071, 7, 51, 68, 19, 273, 61, 21, 46, 74, 41, 1417, 9, 28, 58, 55, 6, 8, 21, 5, 471, 74, 41, 342, 34, 72, 900, 6, 8, 55, 334, 200, 560, 11, 1910, 19, 24, 2186, 7, 15, 16, 12, 14, 15, 16, 12, 14, 60, 33, 345, 5, 289, 744, 9, 2, 905, 6, 77, 13, 58, 2, 2228, 33, 94, 200, 5, 287, 1473, 10, 55, 25, 9, 422, 6, 69, 24, 1182, 2, 325, 47, 95, 49, 80, 83, 5, 592, 11, 141, 55, 27, 5, 1383, 10, 55, 221, 234, 49, 3001, 2, 55, 25, 9, 516, 6, 27, 5, 1383, 10, 42, 10, 55, 1060, 7, 98, 6, 60, 33, 446, 18, 72, 1611, 8, 38, 1102, 214, 11, 1648, 11, 166, 208, 33, 49, 704, 55, 50, 423, 33, 290, 186, 61, 21, 69, 24, 381, 4899, 178, 69, 71, 305, 2182, 18, 48, 2, 6, 69, 24, 226, 2, 8, 62, 74, 323, 52, 48, 2, 11, 851, 18, 9, 423, 2806, 24, 181, 4459, 149, 5, 211, 21, 69, 24, 2064, 11, 7, 15, 16, 12, 14, 15, 16, 12, 14, 2, 138, 36, 1817, 5, 139, 69, 24, 362, 6, 27, 5, 591, 10, 351, 69, 2697, 894, 11, 3887, 7, 17, 24, 5, 1342, 8, 5, 95, 208, 55, 21, 2, 1032, 18, 55, 356, 47, 36, 2885, 11, 404, 55, 745, 8, 155, 69, 138, 62, 69, 138, 7, 19, 148, 186, 552, 2, 21, 69, 24, 291, 18, 2845, 26, 2, 50, 9, 147, 49, 798, 11, 1547, 55, 7, 25, 55, 578, 302, 318, 6, 22, 46, 176, 35, 141, 62, 11, 101, 27, 55, 51, 46, 2953, 55, 263, 8, 17, 569, 55, 7, 22, 19, 281, 18, 21, 27, 44, 72, 540, 7, 15, 16, 12, 14, 15, 16, 12, 14, 371, 18, 2430, 2, 6, 33, 1817, 17, 51, 91, 7]\n",
      "[   0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    1  275 1538   18    2   72  242  139    6   18    5  191\n",
      "  525   21    2 2228 2056   75   18    6   19  158  566  545   10   55   50\n",
      "   72  824    6    2    8  234 1071    7   51   68   19  273   61   21   46\n",
      "   74   41 1417    9   28   58   55    6    8   21    5  471   74   41  342\n",
      "   34   72  900    6    8   55  334  200  560   11 1910   19   24 2186    7\n",
      "   15   16   12   14   15   16   12   14   60   33  345    5  289  744    9\n",
      "    2  905    6   77   13   58    2 2228   33   94  200    5  287 1473   10\n",
      "   55   25    9  422    6   69   24 1182    2  325   47   95   49   80   83\n",
      "    5  592   11  141   55   27    5 1383   10   55  221  234   49 3001    2\n",
      "   55   25    9  516    6   27    5 1383   10   42   10   55 1060    7   98\n",
      "    6   60   33  446   18   72 1611    8   38 1102  214   11 1648   11  166\n",
      "  208   33   49  704   55   50  423   33  290  186   61   21   69   24  381\n",
      " 4899  178   69   71  305 2182   18   48    2    6   69   24  226    2    8\n",
      "   62   74  323   52   48    2   11  851   18    9  423 2806   24  181 4459\n",
      "  149    5  211   21   69   24 2064   11    7   15   16   12   14   15   16\n",
      "   12   14    2  138   36 1817    5  139   69   24  362    6   27    5  591\n",
      "   10  351   69 2697  894   11 3887    7   17   24    5 1342    8    5   95\n",
      "  208   55   21    2 1032   18   55  356   47   36 2885   11  404   55  745\n",
      "    8  155   69  138   62   69  138    7   19  148  186  552    2   21   69\n",
      "   24  291   18 2845   26    2   50    9  147   49  798   11 1547   55    7\n",
      "   25   55  578  302  318    6   22   46  176   35  141   62   11  101   27\n",
      "   55   51   46 2953   55  263    8   17  569   55    7   22   19  281   18\n",
      "   21   27   44   72  540    7   15   16   12   14   15   16   12   14  371\n",
      "   18 2430    2    6   33 1817   17   51   91    7]\n"
     ]
    }
   ],
   "source": [
    "print (X_train[-1])\n",
    "print (X_train_1[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('Build model...')\n",
    "model = Sequential()\n",
    "\n",
    "# we start off with an efficient embedding layer which maps\n",
    "# our vocab indices into embedding_dims dimensions\n",
    "model.add(Embedding(max_features,\n",
    "                    embedding_dims,\n",
    "                    input_length=maxlen,\n",
    "                    dropout=0.2))\n",
    "\n",
    "# we add a Convolution1D, which will learn nb_filter\n",
    "# word group filters of size filter_length:\n",
    "model.add(Convolution1D(nb_filter=nb_filter,\n",
    "                        filter_length=filter_length,\n",
    "                        border_mode='valid',\n",
    "                        activation='relu',\n",
    "                        subsample_length=1))\n",
    "\n",
    "\n",
    "# we use max pooling:\n",
    "model.add(MaxPooling1D())\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "# We add a vanilla hidden layer:\n",
    "model.add(Dense(hidden_dims))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "# We project onto a single unit output layer, and squash it with a sigmoid:\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "model.fit(X_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          nb_epoch=nb_epoch,\n",
    "          validation_data=(X_test, y_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
