{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/home/peng/git/Machine_learning_for_reliability_analysis/Preprocess')\n",
    "sys.path.append('/home/peng/git/Machine_learning_for_reliability_analysis/DAlgorithms')\n",
    "\n",
    "import Preprocessdata\n",
    "import RFclass\n",
    "\n",
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "\n",
    "import logging\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier as RFC\n",
    "\n",
    "\n",
    "from bayes_opt import BayesianOptimization\n",
    "from pandas.core.frame import DataFrame\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics.classification import accuracy_score, confusion_matrix, classification_report\n",
    "from scipy.interpolate import spline\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from sklearn.gaussian_process import GaussianProcess\n",
    "import timeit\n",
    "\n",
    "import seaborn as sns\n",
    "from IPython.core.pylabtools import figsize\n",
    "from scipy.interpolate import spline\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn.cross_validation import StratifiedShuffleSplit\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "import re\n",
    "from astropy.io.fits.header import Header\n",
    "from matplotlib.pyplot import xlim\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            2       3      4       5       6        7        8      9     10  \\\n",
      "0      0.0014  0.0001  100.0  518.67  642.50  1596.95  1407.89  14.62  21.61   \n",
      "1     -0.0008 -0.0001  100.0  518.67  642.70  1589.03  1406.07  14.62  21.61   \n",
      "2     -0.0002  0.0001  100.0  518.67  642.40  1586.97  1402.60  14.62  21.61   \n",
      "3     -0.0031  0.0004  100.0  518.67  642.83  1580.30  1397.26  14.62  21.61   \n",
      "4      0.0010  0.0001  100.0  518.67  643.30  1591.58  1417.72  14.62  21.61   \n",
      "5     -0.0039  0.0002  100.0  518.67  643.60  1602.50  1428.71  14.62  21.61   \n",
      "6     -0.0033  0.0001  100.0  518.67  642.71  1582.08  1401.82  14.62  21.61   \n",
      "7     -0.0025 -0.0001  100.0  518.67  643.59  1593.90  1422.75  14.62  21.61   \n",
      "8      0.0039  0.0001  100.0  518.67  642.27  1594.37  1415.16  14.62  21.61   \n",
      "9      0.0037  0.0001  100.0  518.67  643.57  1595.41  1420.78  14.62  21.61   \n",
      "10     0.0013 -0.0001  100.0  518.67  643.52  1592.63  1412.81  14.62  21.61   \n",
      "11    -0.0034  0.0004  100.0  518.67  642.82  1588.40  1409.10  14.62  21.61   \n",
      "12    -0.0004 -0.0001  100.0  518.67  643.61  1604.10  1429.91  14.62  21.61   \n",
      "13     0.0022 -0.0001  100.0  518.67  642.57  1582.80  1408.74  14.62  21.61   \n",
      "14    -0.0062  0.0004  100.0  518.67  642.31  1578.13  1400.38  14.62  21.61   \n",
      "15    -0.0000  0.0001  100.0  518.67  642.37  1589.72  1414.06  14.62  21.61   \n",
      "16     0.0052 -0.0001  100.0  518.67  643.84  1600.14  1424.05  14.62  21.61   \n",
      "17     0.0024 -0.0003  100.0  518.67  643.93  1604.48  1433.98  14.62  21.61   \n",
      "18     0.0005  0.0003  100.0  518.67  642.52  1586.50  1399.33  14.62  21.60   \n",
      "19    -0.0002 -0.0002  100.0  518.67  642.87  1596.82  1421.71  14.62  21.61   \n",
      "20     0.0003  0.0000  100.0  518.67  643.26  1599.16  1424.96  14.62  21.61   \n",
      "21    -0.0007  0.0001  100.0  518.67  643.02  1588.04  1416.78  14.62  21.61   \n",
      "22    -0.0033  0.0002  100.0  518.67  642.66  1590.94  1407.45  14.62  21.61   \n",
      "23     0.0032  0.0004  100.0  518.67  642.40  1593.49  1407.26  14.62  21.61   \n",
      "24     0.0001 -0.0005  100.0  518.67  643.65  1599.87  1414.22  14.62  21.61   \n",
      "25    -0.0014 -0.0000  100.0  518.67  643.42  1594.76  1424.54  14.62  21.61   \n",
      "26    -0.0050  0.0002  100.0  518.67  643.98  1595.70  1422.52  14.62  21.61   \n",
      "27     0.0014  0.0002  100.0  518.67  642.83  1592.53  1405.34  14.62  21.61   \n",
      "28    -0.0014 -0.0004  100.0  518.67  643.09  1593.53  1415.83  14.62  21.61   \n",
      "29    -0.0021 -0.0003  100.0  518.67  642.49  1587.64  1394.60  14.62  21.61   \n",
      "...       ...     ...    ...     ...     ...      ...      ...    ...    ...   \n",
      "15635 -0.0009 -0.0001  100.0  518.67  643.15  1592.42  1421.28  14.62  21.61   \n",
      "15636  0.0002 -0.0004  100.0  518.67  642.34  1592.02  1402.40  14.62  21.61   \n",
      "15637  0.0009  0.0001  100.0  518.67  642.91  1594.63  1408.26  14.62  21.61   \n",
      "15638 -0.0022  0.0004  100.0  518.67  642.06  1583.79  1404.78  14.62  21.61   \n",
      "15639 -0.0032  0.0004  100.0  518.67  642.82  1584.34  1406.55  14.62  21.61   \n",
      "15640 -0.0012 -0.0000  100.0  518.67  644.01  1596.42  1419.54  14.62  21.61   \n",
      "15641 -0.0003 -0.0005  100.0  518.67  643.35  1603.88  1425.50  14.62  21.61   \n",
      "15642 -0.0015  0.0002  100.0  518.67  642.98  1594.55  1417.41  14.62  21.61   \n",
      "15643  0.0069  0.0001  100.0  518.67  642.38  1590.67  1416.79  14.62  21.61   \n",
      "15644  0.0062  0.0002  100.0  518.67  643.34  1600.71  1415.52  14.62  21.61   \n",
      "15645 -0.0002 -0.0002  100.0  518.67  641.71  1588.01  1397.47  14.62  21.61   \n",
      "15646 -0.0041  0.0001  100.0  518.67  642.75  1587.18  1403.99  14.62  21.61   \n",
      "15647  0.0006  0.0005  100.0  518.67  642.47  1586.53  1401.47  14.62  21.61   \n",
      "15648 -0.0021  0.0001  100.0  518.67  642.62  1592.13  1404.76  14.62  21.61   \n",
      "15649 -0.0038 -0.0002  100.0  518.67  642.33  1592.62  1402.00  14.62  21.61   \n",
      "15650 -0.0026 -0.0004  100.0  518.67  642.19  1585.31  1400.12  14.62  21.61   \n",
      "15651 -0.0008  0.0003  100.0  518.67  643.70  1609.93  1425.93  14.62  21.61   \n",
      "15652  0.0012 -0.0002  100.0  518.67  642.83  1589.06  1409.98  14.62  21.61   \n",
      "15653  0.0008  0.0003  100.0  518.67  642.79  1583.87  1405.43  14.62  21.61   \n",
      "15654  0.0022 -0.0003  100.0  518.67  642.42  1588.44  1404.33  14.62  21.61   \n",
      "15655 -0.0020  0.0001  100.0  518.67  641.65  1584.84  1390.53  14.62  21.61   \n",
      "15656  0.0013  0.0002  100.0  518.67  641.93  1585.47  1403.84  14.62  21.61   \n",
      "15657 -0.0024 -0.0003  100.0  518.67  642.08  1585.06  1403.93  14.62  21.61   \n",
      "15658 -0.0019 -0.0005  100.0  518.67  642.19  1582.64  1401.59  14.62  21.61   \n",
      "15659  0.0008  0.0005  100.0  518.67  643.51  1595.33  1413.99  14.62  21.61   \n",
      "15660  0.0025  0.0003  100.0  518.67  642.06  1584.05  1401.99  14.62  21.61   \n",
      "15661 -0.0011 -0.0002  100.0  518.67  642.71  1593.21  1406.15  14.62  21.61   \n",
      "15662  0.0021 -0.0003  100.0  518.67  643.28  1595.16  1426.14  14.62  21.61   \n",
      "15663 -0.0020 -0.0001  100.0  518.67  642.01  1585.44  1401.33  14.62  21.61   \n",
      "15664  0.0004 -0.0001  100.0  518.67  642.08  1577.94  1397.09  14.62  21.61   \n",
      "\n",
      "           11     ...            17       18      19    20   21    22     23  \\\n",
      "0      553.84     ...       2388.09  8138.33  8.3871  0.03  392  2388  100.0   \n",
      "1      553.74     ...       2388.06  8188.65  8.4168  0.03  393  2388  100.0   \n",
      "2      553.92     ...       2387.99  8148.55  8.4122  0.03  391  2388  100.0   \n",
      "3      554.61     ...       2388.03  8132.62  8.3891  0.03  391  2388  100.0   \n",
      "4      552.80     ...       2388.18  8147.14  8.4722  0.03  395  2388  100.0   \n",
      "5      552.20     ...       2388.24  8137.00  8.5474  0.03  394  2388  100.0   \n",
      "6      553.90     ...       2388.00  8144.22  8.4266  0.03  392  2388  100.0   \n",
      "7      551.98     ...       2388.26  8125.82  8.5017  0.03  396  2388  100.0   \n",
      "8      552.65     ...       2388.10  8172.57  8.4662  0.03  393  2388  100.0   \n",
      "9      552.70     ...       2388.23  8110.72  8.4846  0.03  394  2388  100.0   \n",
      "10     553.34     ...       2388.11  8145.31  8.4221  0.03  394  2388  100.0   \n",
      "11     553.25     ...       2388.09  8127.34  8.4032  0.03  392  2388  100.0   \n",
      "12     551.25     ...       2388.23  8176.82  8.5502  0.03  398  2388  100.0   \n",
      "13     553.95     ...       2388.11  8134.36  8.4271  0.03  393  2388  100.0   \n",
      "14     553.93     ...       2388.04  8144.68  8.4037  0.03  393  2388  100.0   \n",
      "15     553.66     ...       2388.16  8122.65  8.4372  0.03  393  2388  100.0   \n",
      "16     551.15     ...       2388.28  8124.23  8.5268  0.03  396  2388  100.0   \n",
      "17     551.88     ...       2388.32  8121.62  8.4912  0.03  396  2388  100.0   \n",
      "18     554.09     ...       2388.00  8162.33  8.4074  0.03  391  2388  100.0   \n",
      "19     552.73     ...       2388.10  8201.98  8.4766  0.03  396  2388  100.0   \n",
      "20     550.83     ...       2388.29  8132.37  8.5279  0.03  397  2388  100.0   \n",
      "21     552.15     ...       2388.15  8141.20  8.4487  0.03  393  2388  100.0   \n",
      "22     553.33     ...       2388.12  8140.33  8.4280  0.03  391  2388  100.0   \n",
      "23     553.64     ...       2388.07  8139.37  8.4069  0.03  393  2388  100.0   \n",
      "24     552.57     ...       2388.11  8150.60  8.4369  0.03  395  2388  100.0   \n",
      "25     552.03     ...       2388.27  8127.18  8.4748  0.03  394  2388  100.0   \n",
      "26     551.47     ...       2388.20  8146.19  8.4757  0.03  395  2388  100.0   \n",
      "27     553.31     ...       2388.09  8126.02  8.4134  0.03  395  2388  100.0   \n",
      "28     552.14     ...       2388.04  8180.78  8.4833  0.03  394  2388  100.0   \n",
      "29     554.43     ...       2388.07  8133.19  8.3839  0.03  391  2388  100.0   \n",
      "...       ...     ...           ...      ...     ...   ...  ...   ...    ...   \n",
      "15635  552.36     ...       2388.16  8127.80  8.4650  0.03  394  2388  100.0   \n",
      "15636  554.18     ...       2388.07  8145.67  8.4605  0.03  392  2388  100.0   \n",
      "15637  552.96     ...       2388.11  8149.71  8.4306  0.03  393  2388  100.0   \n",
      "15638  553.90     ...       2388.04  8159.92  8.4498  0.03  392  2388  100.0   \n",
      "15639  553.81     ...       2388.07  8142.00  8.3963  0.03  393  2388  100.0   \n",
      "15640  552.05     ...       2388.17  8133.30  8.5006  0.03  397  2388  100.0   \n",
      "15641  552.22     ...       2388.28  8146.60  8.5170  0.03  395  2388  100.0   \n",
      "15642  551.98     ...       2388.23  8123.03  8.4754  0.03  394  2388  100.0   \n",
      "15643  552.35     ...       2388.19  8135.58  8.4676  0.03  394  2388  100.0   \n",
      "15644  552.41     ...       2388.20  8158.34  8.5128  0.03  394  2388  100.0   \n",
      "15645  554.19     ...       2387.99  8152.41  8.4104  0.03  392  2388  100.0   \n",
      "15646  553.80     ...       2388.15  8128.78  8.4271  0.03  393  2388  100.0   \n",
      "15647  553.85     ...       2388.08  8143.29  8.4334  0.03  392  2388  100.0   \n",
      "15648  553.51     ...       2388.05  8134.90  8.4490  0.03  395  2388  100.0   \n",
      "15649  553.46     ...       2388.09  8132.40  8.4321  0.03  393  2388  100.0   \n",
      "15650  554.53     ...       2387.98  8139.44  8.3833  0.03  393  2388  100.0   \n",
      "15651  551.96     ...       2388.18  8189.64  8.5305  0.03  396  2388  100.0   \n",
      "15652  553.06     ...       2388.10  8125.35  8.4399  0.03  394  2388  100.0   \n",
      "15653  553.59     ...       2388.12  8139.05  8.4359  0.03  392  2388  100.0   \n",
      "15654  553.53     ...       2388.06  8140.72  8.4988  0.03  392  2388  100.0   \n",
      "15655  554.50     ...       2388.01  8141.59  8.3869  0.03  392  2388  100.0   \n",
      "15656  553.38     ...       2388.01  8137.92  8.4015  0.03  394  2388  100.0   \n",
      "15657  553.77     ...       2387.99  8134.52  8.4200  0.03  393  2388  100.0   \n",
      "15658  553.97     ...       2388.07  8140.01  8.4185  0.03  391  2388  100.0   \n",
      "15659  552.03     ...       2388.17  8140.47  8.4821  0.03  396  2388  100.0   \n",
      "15660  553.99     ...       2388.05  8154.57  8.4353  0.03  392  2388  100.0   \n",
      "15661  554.28     ...       2388.00  8183.83  8.4203  0.03  394  2388  100.0   \n",
      "15662  551.52     ...       2388.26  8134.69  8.5076  0.03  395  2388  100.0   \n",
      "15663  554.81     ...       2387.99  8135.05  8.3603  0.03  392  2388  100.0   \n",
      "15664  554.33     ...       2388.00  8145.89  8.4318  0.03  392  2388  100.0   \n",
      "\n",
      "          24       25  target_easy  \n",
      "0      39.07  23.4434          1.0  \n",
      "1      38.93  23.3320          2.0  \n",
      "2      39.06  23.4204          0.0  \n",
      "3      39.10  23.4014          0.0  \n",
      "4      38.73  23.2368          2.0  \n",
      "5      38.35  23.0499          2.0  \n",
      "6      38.89  23.4144          0.0  \n",
      "7      38.60  23.1241          2.0  \n",
      "8      38.72  23.2645          2.0  \n",
      "9      38.50  23.1192          2.0  \n",
      "10     38.66  23.3003          1.0  \n",
      "11     38.88  23.3197          0.0  \n",
      "12     38.45  22.9884          2.0  \n",
      "13     38.88  23.3326          0.0  \n",
      "14     39.00  23.4549          0.0  \n",
      "15     38.66  23.3204          0.0  \n",
      "16     38.58  23.0729          2.0  \n",
      "17     38.44  23.0578          2.0  \n",
      "18     38.89  23.3686          1.0  \n",
      "19     38.59  23.1856          2.0  \n",
      "20     38.29  23.1123          2.0  \n",
      "21     38.94  23.2748          2.0  \n",
      "22     38.88  23.2912          1.0  \n",
      "23     38.96  23.3085          0.0  \n",
      "24     38.75  23.1790          2.0  \n",
      "25     38.74  23.1146          2.0  \n",
      "26     38.52  23.0871          2.0  \n",
      "27     38.77  23.3261          1.0  \n",
      "28     38.86  23.1733          2.0  \n",
      "29     39.15  23.3759          0.0  \n",
      "...      ...      ...          ...  \n",
      "15635  38.70  23.2318          2.0  \n",
      "15636  39.00  23.2933          1.0  \n",
      "15637  38.58  23.2276          1.0  \n",
      "15638  38.99  23.3608          1.0  \n",
      "15639  38.78  23.3514          0.0  \n",
      "15640  38.57  23.1864          2.0  \n",
      "15641  38.44  23.0762          2.0  \n",
      "15642  38.56  23.2132          2.0  \n",
      "15643  38.76  23.1019          1.0  \n",
      "15644  38.55  23.0728          2.0  \n",
      "15645  38.72  23.3992          1.0  \n",
      "15646  38.76  23.3394          1.0  \n",
      "15647  38.80  23.2273          1.0  \n",
      "15648  39.04  23.3601          1.0  \n",
      "15649  38.93  23.2351          0.0  \n",
      "15650  38.93  23.4145          0.0  \n",
      "15651  38.34  23.0647          2.0  \n",
      "15652  38.91  23.2996          0.0  \n",
      "15653  38.79  23.3032          1.0  \n",
      "15654  38.96  23.3602          0.0  \n",
      "15655  38.93  23.4954          0.0  \n",
      "15656  39.02  23.3915          1.0  \n",
      "15657  38.88  23.2940          1.0  \n",
      "15658  38.92  23.3359          0.0  \n",
      "15659  38.58  23.1788          2.0  \n",
      "15660  38.88  23.4948          1.0  \n",
      "15661  38.74  23.2732          2.0  \n",
      "15662  38.44  23.0835          2.0  \n",
      "15663  38.86  23.3726          0.0  \n",
      "15664  38.99  23.3886          1.0  \n",
      "\n",
      "[15665 rows x 25 columns]\n"
     ]
    }
   ],
   "source": [
    "# Load data set and target values\n",
    "\n",
    "start = timeit.default_timer()\n",
    "p= Preprocessdata.standardprocess()\n",
    "\n",
    "\n",
    "save_path = '/home/peng/Documents/Project_C/Turbofan/'\n",
    "#names = xrange(0,28)\n",
    "\n",
    "df = pd.read_csv(save_path + 'train_clean_3_disc_labels_01.txt', header=0)\n",
    "\n",
    "rad_stat = 2\n",
    "seed = 'rf'\n",
    "\n",
    "# drop the first column\n",
    "df = df.drop(df.columns[[0]], axis=1)\n",
    "\n",
    "print (df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            6        7        8      11     15      16       19  target_easy\n",
      "0      537.23  1267.05  1048.12  175.22  36.80  164.07  10.9091          2.0\n",
      "1      608.58  1495.48  1268.96  334.06  44.78  314.36   9.3192          3.0\n",
      "2      550.50  1364.34  1138.45  138.23  42.34  130.11   9.4250          3.0\n",
      "3      642.25  1585.29  1398.16  554.17  47.13  521.95   8.4106          1.0\n",
      "4      549.70  1347.90  1129.64  138.17  42.07  130.35   9.3709          0.0\n",
      "5      642.81  1587.46  1415.79  554.24  47.52  521.27   8.4327          2.0\n",
      "6      556.60  1385.96  1150.47  194.41  42.67  182.61   9.4489          3.0\n",
      "7      642.65  1593.32  1401.55  554.14  47.45  521.89   8.4000          1.0\n",
      "8      550.55  1366.40  1145.30  138.42  42.64  130.21   9.4611          3.0\n",
      "9      607.87  1488.99  1252.58  334.34  44.51  314.08   9.2530          2.0\n",
      "10     608.21  1490.72  1273.90  333.21  45.00  313.77   9.3166          3.0\n",
      "11     607.24  1490.66  1258.75  334.82  44.31  314.42   9.2587          2.0\n",
      "12     643.26  1605.82  1433.93  551.75  48.00  518.93   8.5156          3.0\n",
      "13     605.65  1503.00  1325.90  393.41  45.97  371.15   8.7075          3.0\n",
      "14     641.90  1585.72  1393.27  554.24  47.02  522.72   8.3726          1.0\n",
      "15     536.56  1256.05  1045.72  176.05  36.52  165.01  10.8561          1.0\n",
      "16     605.24  1500.97  1314.12  393.93  45.60  371.38   8.6436          2.0\n",
      "17     604.54  1494.36  1296.99  395.14  45.06  371.85   8.5790          0.0\n",
      "18     607.39  1493.32  1259.28  333.92  44.64  314.12   9.2888          3.0\n",
      "19     607.84  1488.84  1255.71  333.50  44.65  314.80   9.2348          3.0\n",
      "20     556.25  1372.95  1140.68  193.95  42.17  182.77   9.3612          3.0\n",
      "21     643.34  1589.00  1412.89  553.06  47.55  521.70   8.4570          1.0\n",
      "22     556.50  1377.20  1154.16  193.17  42.54  182.30   9.4289          3.0\n",
      "23     607.12  1485.57  1260.01  335.04  44.57  314.72   9.2277          2.0\n",
      "24     604.27  1502.14  1305.56  394.84  45.17  372.11   8.6102          2.0\n",
      "25     536.82  1265.02  1045.81  174.54  36.81  164.57  10.9145          2.0\n",
      "26     643.04  1589.39  1416.74  553.30  47.74  521.19   8.4991          2.0\n",
      "27     549.10  1358.30  1114.92  138.43  41.94  130.51   9.4062          1.0\n",
      "28     549.70  1356.71  1135.40  138.37  42.35  130.05   9.3870          2.0\n",
      "29     555.72  1368.13  1133.87  194.83  42.20  182.20   9.3704          2.0\n",
      "...       ...      ...      ...     ...    ...     ...      ...          ...\n",
      "33526  549.73  1357.49  1126.22  138.47  41.86  130.35   9.3494          1.0\n",
      "33527  550.35  1360.12  1145.11  138.35  42.61  130.65   9.4101          3.0\n",
      "33528  549.30  1347.66  1111.80  138.73  41.81  130.90   9.3068          0.0\n",
      "33529  642.44  1582.44  1410.85  553.47  47.37  521.93   8.4090          2.0\n",
      "33530  536.65  1257.92  1045.64  175.19  36.55  164.79  10.9253          2.0\n",
      "33531  643.20  1600.05  1422.90  551.11  47.84  520.18   8.5462          3.0\n",
      "33532  550.38  1368.23  1132.96  138.34  42.72  130.54   9.4009          3.0\n",
      "33533  536.96  1259.11  1058.12  175.61  36.63  164.33  10.9529          2.0\n",
      "33534  604.79  1499.99  1314.69  394.74  45.44  371.84   8.6712          2.0\n",
      "33535  642.69  1591.21  1397.41  554.25  47.34  521.58   8.3785          2.0\n",
      "33536  550.49  1357.40  1138.27  138.58  42.75  130.44   9.4633          3.0\n",
      "33537  605.40  1500.24  1321.49  394.35  45.38  370.92   8.7061          3.0\n",
      "33538  605.00  1504.97  1318.29  393.65  45.75  371.80   8.6648          3.0\n",
      "33539  606.36  1513.89  1327.10  392.75  46.18  370.49   8.7426          3.0\n",
      "33540  606.10  1514.25  1325.47  393.80  45.95  370.37   8.7490          3.0\n",
      "33541  605.22  1498.30  1309.04  393.53  45.39  371.05   8.6767          1.0\n",
      "33542  536.13  1261.41  1052.38  175.03  36.90  164.53  10.9024          2.0\n",
      "33543  642.44  1585.91  1408.90  553.88  47.38  521.12   8.4235          1.0\n",
      "33544  555.66  1375.92  1145.61  193.95  42.21  182.82   9.3969          3.0\n",
      "33545  549.45  1349.57  1118.29  139.02  41.97  130.47   9.3296          1.0\n",
      "33546  607.72  1483.11  1255.58  333.90  44.66  314.98   9.2714          2.0\n",
      "33547  607.21  1485.04  1245.33  334.82  44.23  315.46   9.1932          2.0\n",
      "33548  536.59  1251.30  1042.00  175.40  36.45  164.42  10.8562          1.0\n",
      "33549  642.00  1591.83  1406.91  553.81  47.43  521.95   8.4279          1.0\n",
      "33550  604.42  1495.12  1298.91  394.98  45.03  372.32   8.6674          1.0\n",
      "33551  606.91  1480.49  1243.14  334.26  44.28  315.82   9.1909          1.0\n",
      "33552  537.19  1268.47  1052.88  175.16  36.80  164.10  10.9218          2.0\n",
      "33553  642.67  1597.13  1406.22  553.17  47.85  521.48   8.4498          3.0\n",
      "33554  605.58  1509.43  1326.17  393.65  45.90  370.68   8.7219          3.0\n",
      "33555  604.68  1502.70  1307.19  394.71  45.31  371.49   8.6414          2.0\n",
      "\n",
      "[33556 rows x 8 columns]\n"
     ]
    }
   ],
   "source": [
    "# select 7 out of the 24\n",
    "\n",
    "df_7 = df[[ 4 ,5, 6, 9, 13, 14, 17, 24]]\n",
    "\n",
    "print (df_7)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#### scale the training dataset first and then apply the rules to test set \n",
    "train, trainlabel, test, testlabel = p.sep_scale_divd(df, 0.7)\n",
    "#print (test[0:10])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## PCA: reduce the dimensions\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca_fit = PCA(n_components=6).fit(train)\n",
    "pca_train = pca_fit.transform(train)\n",
    "pca_test = pca_fit.transform(test)\n",
    "\n",
    "train = pca_train\n",
    "test = pca_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# optional: binary the label\n",
    "\n",
    "from sklearn.preprocessing import label_binarize\n",
    "\n",
    "trainlabel = label_binarize(trainlabel, classes = [0,1,2,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### try autoencoding here 1. layout of the NN\n",
    "\n",
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model\n",
    "from keras import regularizers\n",
    "\n",
    "# this is the size of our encoded representations\n",
    "encoding_dim = 6  # 32 floats -> compression of factor 24.5, assuming the input is 784 floats\n",
    "\n",
    "# this is our input placeholder\n",
    "input_dim= Input(shape=(24,))\n",
    "# \"encoded\" is the encoded representation of the input\n",
    "encoded = Dense(20, activation='tanh')(input_dim)\n",
    "encoded = Dense(12, activation='tanh')(encoded)\n",
    "encoded = Dense(18, activation='sigmoid')(encoded)\n",
    "encoded = Dense(12, activation='tanh')(encoded)\n",
    "encoded = Dense(18, activation='sigmoid')(encoded)\n",
    "encoded = Dense(20, activation='sigmoid',  activity_regularizer=regularizers.activity_l1(10e-4))(encoded)\n",
    "\n",
    "decoded = Dense(24, activation='sigmoid')(encoded)\n",
    "\n",
    "\n",
    "# this model maps an input to its reconstruction\n",
    "autoencoder = Model(input=input_dim, output=decoded)\n",
    "\n",
    "encoder = Model(input=input_dim, output=encoded)\n",
    "autoencoder.compile(optimizer='sgd', loss='mse')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "7000/7000 [==============================] - 0s - loss: 0.1264     \n",
      "Epoch 2/50\n",
      "7000/7000 [==============================] - 0s - loss: 0.1256     \n",
      "Epoch 3/50\n",
      "7000/7000 [==============================] - 0s - loss: 0.1247     \n",
      "Epoch 4/50\n",
      "7000/7000 [==============================] - 0s - loss: 0.1239     \n",
      "Epoch 5/50\n",
      "7000/7000 [==============================] - 0s - loss: 0.1230     \n",
      "Epoch 6/50\n",
      "7000/7000 [==============================] - 0s - loss: 0.1222     \n",
      "Epoch 7/50\n",
      "7000/7000 [==============================] - 0s - loss: 0.1214     \n",
      "Epoch 8/50\n",
      "7000/7000 [==============================] - 0s - loss: 0.1206     \n",
      "Epoch 9/50\n",
      "7000/7000 [==============================] - 0s - loss: 0.1198     \n",
      "Epoch 10/50\n",
      "7000/7000 [==============================] - 0s - loss: 0.1190     \n",
      "Epoch 11/50\n",
      "7000/7000 [==============================] - 0s - loss: 0.1182     \n",
      "Epoch 12/50\n",
      "7000/7000 [==============================] - 0s - loss: 0.1174     \n",
      "Epoch 13/50\n",
      "7000/7000 [==============================] - 0s - loss: 0.1166     \n",
      "Epoch 14/50\n",
      "7000/7000 [==============================] - 0s - loss: 0.1158     \n",
      "Epoch 15/50\n",
      "7000/7000 [==============================] - 0s - loss: 0.1151     \n",
      "Epoch 16/50\n",
      "7000/7000 [==============================] - 0s - loss: 0.1143     \n",
      "Epoch 17/50\n",
      "7000/7000 [==============================] - 0s - loss: 0.1136     \n",
      "Epoch 18/50\n",
      "7000/7000 [==============================] - 0s - loss: 0.1128     \n",
      "Epoch 19/50\n",
      "7000/7000 [==============================] - 0s - loss: 0.1121     \n",
      "Epoch 20/50\n",
      "7000/7000 [==============================] - 0s - loss: 0.1114     \n",
      "Epoch 21/50\n",
      "7000/7000 [==============================] - 0s - loss: 0.1106     \n",
      "Epoch 22/50\n",
      "7000/7000 [==============================] - 0s - loss: 0.1099     \n",
      "Epoch 23/50\n",
      "7000/7000 [==============================] - 0s - loss: 0.1092     \n",
      "Epoch 24/50\n",
      "7000/7000 [==============================] - 0s - loss: 0.1085     \n",
      "Epoch 25/50\n",
      "7000/7000 [==============================] - 0s - loss: 0.1078     \n",
      "Epoch 26/50\n",
      "7000/7000 [==============================] - 0s - loss: 0.1071     \n",
      "Epoch 27/50\n",
      "7000/7000 [==============================] - 0s - loss: 0.1064     \n",
      "Epoch 28/50\n",
      "7000/7000 [==============================] - 0s - loss: 0.1058     \n",
      "Epoch 29/50\n",
      "7000/7000 [==============================] - 0s - loss: 0.1051     \n",
      "Epoch 30/50\n",
      "7000/7000 [==============================] - 0s - loss: 0.1044     \n",
      "Epoch 31/50\n",
      "7000/7000 [==============================] - 0s - loss: 0.1038     \n",
      "Epoch 32/50\n",
      "7000/7000 [==============================] - 0s - loss: 0.1031     \n",
      "Epoch 33/50\n",
      "7000/7000 [==============================] - 0s - loss: 0.1025     \n",
      "Epoch 34/50\n",
      "7000/7000 [==============================] - 0s - loss: 0.1018     \n",
      "Epoch 35/50\n",
      "7000/7000 [==============================] - 0s - loss: 0.1012     \n",
      "Epoch 36/50\n",
      "7000/7000 [==============================] - 0s - loss: 0.1006     \n",
      "Epoch 37/50\n",
      "7000/7000 [==============================] - 0s - loss: 0.0999     \n",
      "Epoch 38/50\n",
      "7000/7000 [==============================] - 0s - loss: 0.0993     \n",
      "Epoch 39/50\n",
      "7000/7000 [==============================] - 0s - loss: 0.0987     \n",
      "Epoch 40/50\n",
      "7000/7000 [==============================] - 0s - loss: 0.0981     \n",
      "Epoch 41/50\n",
      "7000/7000 [==============================] - 0s - loss: 0.0975     \n",
      "Epoch 42/50\n",
      "7000/7000 [==============================] - 0s - loss: 0.0969     \n",
      "Epoch 43/50\n",
      "7000/7000 [==============================] - 0s - loss: 0.0963     \n",
      "Epoch 44/50\n",
      "7000/7000 [==============================] - 0s - loss: 0.0958     \n",
      "Epoch 45/50\n",
      "7000/7000 [==============================] - 0s - loss: 0.0952     \n",
      "Epoch 46/50\n",
      "7000/7000 [==============================] - 0s - loss: 0.0946     \n",
      "Epoch 47/50\n",
      "7000/7000 [==============================] - 0s - loss: 0.0941     \n",
      "Epoch 48/50\n",
      "7000/7000 [==============================] - 0s - loss: 0.0935     \n",
      "Epoch 49/50\n",
      "7000/7000 [==============================] - 0s - loss: 0.0930     \n",
      "Epoch 50/50\n",
      "7000/7000 [==============================] - 0s - loss: 0.0924     \n",
      "The running takes 0.05048148632049561 min\n"
     ]
    }
   ],
   "source": [
    "start = timeit.default_timer()\n",
    "autoencoder.fit(train, train,\n",
    "                nb_epoch=50,\n",
    "                batch_size=500,\n",
    "                shuffle=True\n",
    "                )\n",
    "\n",
    "stop = timeit.default_timer()\n",
    "\n",
    "print (\"The running takes %r min\" %((stop-start)/60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7000, 20)\n",
      "(3000, 20)\n"
     ]
    }
   ],
   "source": [
    "train = encoder.predict(train)\n",
    "print(np.shape(train))\n",
    "test= encoder.predict(test)\n",
    "print(np.shape(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The running takes 0.003956401348114013 min\n"
     ]
    }
   ],
   "source": [
    "# fit the rf forest\n",
    "\n",
    "start = timeit.default_timer()\n",
    "classifier = RFC()\n",
    "             \n",
    "\n",
    "classifier.fit(train, trainlabel)\n",
    "\n",
    "stop = timeit.default_timer()\n",
    "\n",
    "print (\"The running takes %r min\" %((stop-start)/60))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1173  575   11]\n",
      " [ 330  952  142]\n",
      " [   4   79 1434]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.67      0.78      0.72      1507\n",
      "        1.0       0.67      0.59      0.63      1606\n",
      "        2.0       0.95      0.90      0.92      1587\n",
      "\n",
      "avg / total       0.76      0.76      0.76      4700\n",
      "\n",
      "0.757234042553\n"
     ]
    }
   ],
   "source": [
    "#predict the test set with AE\n",
    "\n",
    "#tt=RFclass.test()\n",
    "#result = tt.testforest_score(test, testlabel, classifier)\n",
    "\n",
    "outputtest = classifier.predict(test)\n",
    "print (confusion_matrix(outputtest,testlabel))\n",
    "print( classification_report(testlabel, outputtest))\n",
    "print ( accuracy_score(testlabel, outputtest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2439 1477  112]\n",
      " [ 970 1821  517]\n",
      " [  51  354 3011]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.61      0.70      0.65      3460\n",
      "        1.0       0.55      0.50      0.52      3652\n",
      "        2.0       0.88      0.83      0.85      3640\n",
      "\n",
      "avg / total       0.68      0.68      0.68     10752\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#predict the test set\n",
    "\n",
    "#tt=RFclass.test()\n",
    "#result = tt.testforest_score(test, testlabel, classifier)\n",
    "\n",
    "outputtest = classifier.predict(test)\n",
    "print (confusion_matrix(outputtest,testlabel))\n",
    "print( classification_report(testlabel, outputtest))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
